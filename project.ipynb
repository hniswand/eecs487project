{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-graduation",
   "metadata": {
    "id": "bored-graduation"
   },
   "source": [
    "# EECS 487 Project: Naive Bayes Classifier of Sentiment Analysis of Contraseptives\n",
    "\n",
    "This notebook contains the code of our project. In the second problem, you will build naive bayes classifiers to distinguish between legitimate news headlines and clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "angry-large",
   "metadata": {
    "id": "angry-large"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/hannahniswander/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/hannahniswander/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/hannahniswander/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/hannahniswander/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/hannahniswander/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --upgrade  # after running this line once, you can comment this line out\n",
    "import nltk\n",
    "print(nltk.__version__) # this should print out 3.8.1 if you have installed the latest version of NLTK properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ec2d2",
   "metadata": {
    "id": "6e6ec2d2"
   },
   "source": [
    "Before we get started, run the following cell to load the autoreload extension so that functions in ```language_model.py``` and ```naive_bayes.py``` will be re-imported into the notebook every time we run them. We also need to import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ca6b2e",
   "metadata": {
    "id": "94ca6b2e"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from language_model import *\n",
    "from naive_bayes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-poison",
   "metadata": {
    "id": "regular-poison"
   },
   "source": [
    "## C.2 Naive Bayes for Text Classification [26 points]\n",
    "In this problem, you will build naive bayes classifiers to do text classification. You will use the clickbait headlines dataset, which contains examples of legitimate news headlines and clickbait news headlines. The original dataset can be found in [this GitHub repository](https://github.com/bhargaviparanjape/clickbait) and [this paper](https://arxiv.org/abs/1610.09786).\n",
    "### C.2.1 Load dataset [4 points]\n",
    "To get started, **fill in** the function ```load_headlines``` to load the clickbait dataset into pandas dataframes. The file ```clickbait_data.csv``` contains a partially processed subset of the data. It contains two columns: (1) ```is_clickbait``` is 1 when the row contains a clickbait headline and 0 when it doesn't and (2) ```text```, which contains the headline itself.\n",
    "\n",
    "To get started, **fill in** the function ```load_headlines``` to load the clickbait dataset into a pandas dataframe. To do this, you will need to do the following:\n",
    "\n",
    "1. Read in the ```text``` and ```is_clickbait``` columns.\n",
    "2. Rename the ```is_clickbait``` column to ```label```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-execution",
   "metadata": {
    "id": "realistic-execution"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>Watch These Grown-Ups Freak Out Over Virtual P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>What Your Pinterest Account Looks Like Right B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>23 Dance Moves That Changed Our Lives In 2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7787</th>\n",
       "      <td>China Accuses Dissident of Subversion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>6.2 magnitude earthquake hits northern Chile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>16 New Year's Resolutions That Will Make Anyon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>After Her Photographer Was A No-Show For Her W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>We Need To Talk About Snoke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>Show Us Your Colorful Tattoos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>17 Things \"Gossip Girl\" Actually Got Right Abo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "1102  Watch These Grown-Ups Freak Out Over Virtual P...      1\n",
       "3019  What Your Pinterest Account Looks Like Right B...      1\n",
       "2768      23 Dance Moves That Changed Our Lives In 2015      1\n",
       "7787              China Accuses Dissident of Subversion      0\n",
       "9772       6.2 magnitude earthquake hits northern Chile      0\n",
       "...                                                 ...    ...\n",
       "3347  16 New Year's Resolutions That Will Make Anyon...      1\n",
       "4222  After Her Photographer Was A No-Show For Her W...      1\n",
       "2392                        We Need To Talk About Snoke      1\n",
       "4884                      Show Us Your Colorful Tattoos      1\n",
       "941   17 Things \"Gossip Girl\" Actually Got Right Abo...      1\n",
       "\n",
       "[9000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>New Status in Africa Empowers an Ever-Eccentri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>The New Trailer For \"Orange Is The New Black\" ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8249</th>\n",
       "      <td>Chopper knocks Austrian gondola off cables, ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>Carrie-Anne Moss Isn't Aging And It Must Be Magic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>21 People Who Took Passive-Aggressive To A New...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7687</th>\n",
       "      <td>Alaska senator Ted Stevens indicted in corrupt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>This Clydesdale Horse Already Has The Best Pho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6120</th>\n",
       "      <td>Matthew Edwards, honored Michigan police offic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>23 Of The Funniest \"Nancy Drew\" Game Memes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9110</th>\n",
       "      <td>Judge Halts Investigation Into ID Theft in Col...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "6523  New Status in Africa Empowers an Ever-Eccentri...      0\n",
       "231   The New Trailer For \"Orange Is The New Black\" ...      1\n",
       "8249  Chopper knocks Austrian gondola off cables, ki...      0\n",
       "1540  Carrie-Anne Moss Isn't Aging And It Must Be Magic      1\n",
       "311   21 People Who Took Passive-Aggressive To A New...      1\n",
       "...                                                 ...    ...\n",
       "7687  Alaska senator Ted Stevens indicted in corrupt...      0\n",
       "831   This Clydesdale Horse Already Has The Best Pho...      1\n",
       "6120  Matthew Edwards, honored Michigan police offic...      0\n",
       "2958         23 Of The Funniest \"Nancy Drew\" Game Memes      1\n",
       "9110  Judge Halts Investigation Into ID Theft in Col...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from naive_bayes import *\n",
    "\n",
    "all_data = load_headlines('clickbait_data.csv')\n",
    "\n",
    "(train, test) = train_test_split(all_data, train_size=0.9)\n",
    "\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-spank",
   "metadata": {
    "id": "earlier-spank"
   },
   "source": [
    "### C.2.2 Dataset statistics [3 points]\n",
    "Before start training classifiers, you need to calculate some basic statistics of the dataset. **Fill in** the function ```get_basic_stats``` to print out the following statistics of the training data:\n",
    "- Average number of tokens per headline\n",
    "- Standard deviation of the number of tokens per headline\n",
    "- Total number of legitimate headlines\n",
    "- Total number of clickbait headlines\n",
    "\n",
    "Note: you can use any tokenization method you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-account",
   "metadata": {
    "id": "sticky-account"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per headline: 9.64188888888889\n",
      "Standard deviation: 2.958355547125792\n",
      "Number of legitimate/clickbait headlines: {0: 4488, 1: 4512}\n"
     ]
    }
   ],
   "source": [
    "get_basic_stats(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-rental",
   "metadata": {
    "id": "exterior-rental"
   },
   "source": [
    "### C.2.3 Data processing and ngram calculation [6 points]\n",
    "Now you need to calculate the ngram counts. **Fill in** the function ```fit``` that, given a dataframe of training data, calculates the ngram counts in each category and the prior probability for each category. Concretely, **store** the total occurrence of each ngram in each category in a list called ```self.ngram_count``` so that ```self.ngram_count[0]``` contains $count(w, c_0)$ for all $w$ in the vocabulary, and ```self.ngram_count[1]``` contains $count(w, c_1)$, etc. ```self.ngram_count[i]``` should be an array of shape $(1,|V|)$, where $V$ is the vocabulary (total vocabulary across both classes). **Store** the total occurrence of all ngrams in each category in a list called ```self.total_count``` so that ```self.total_count[0]``` $=\\sum_{w\\in V}count(w, c_0)$, and ```self.total_count[1]``` $=\\sum_{w\\in V}count(w, c_1)$, etc. **Store** the prior probability for each category in ```self.category_prob```. You need to follow these rules when calculating the counts:\n",
    "- convert all letters to lowercase;\n",
    "- include both unigrams and bigrams;\n",
    "- ignore terms that appear in more than 80\\% of the headlines;\n",
    "- ignore terms that appear in less than 3 headlines.\n",
    "\n",
    "Hint: use ```CountVectorizer``` in sklearn and store it as ```self.vectorizer```. You need to use **both legitimate and clickbait headlines** to get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "personalized-opening",
   "metadata": {
    "id": "personalized-opening"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for each category: [0.49866667 0.50133333]\n",
      "Length of self.ngram_count: 2\n",
      "Shape of the counts for 1st category: (7275,)\n",
      "Number of non-zero terms for 1st category: 4763\n",
      "Maximum count of the 1st category: 1280.0\n",
      "Minimum count of the 1st category: 0.0\n",
      "Sum of ngram count for 1st category: 35568.0\n",
      "Total count for each category: [35568. 56836.]\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes.fit(train)\n",
    "print(f\"Probability for each category: {naive_bayes.category_prob}\")\n",
    "print(f\"Length of self.ngram_count: {len(naive_bayes.ngram_count)}\")\n",
    "print(f\"Shape of the counts for 1st category: {naive_bayes.ngram_count[0].shape}\")\n",
    "print(f\"Number of non-zero terms for 1st category: {(naive_bayes.ngram_count[0] > 0).sum()}\")\n",
    "print(f\"Maximum count of the 1st category: {naive_bayes.ngram_count[0].max()}\")\n",
    "print(f\"Minimum count of the 1st category: {naive_bayes.ngram_count[0].min()}\")\n",
    "print(f\"Sum of ngram count for 1st category: {naive_bayes.ngram_count[0].sum()}\")\n",
    "print(f\"Total count for each category: {naive_bayes.total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-decimal",
   "metadata": {
    "id": "blind-decimal"
   },
   "source": [
    "### C.2.4 Calculate posterior probability for a category [4 points]\n",
    "Next, you will use the vectorizer and ngram counts to calculate the posterior probability of a category. In this homework, we have two categories: legitimate and clickbait. **Fill in** the function ```calculate_prob``` that given a list of articles $docs$, a category index $i$, return $\\log\\left(p(c_i)p(d|c_i)\\right)=\\log\\left(p(c_i)\\prod_{x\\in X}p(x|c_i)\\right)$ for each article $d$ in $docs$, where $X$ is the set of unigrams and bigrams in **both** article $d$ and vocabulary $V$.\n",
    "\n",
    "- Use **add-one smoothing** in your calculation.\n",
    "- Simply discard unseen unigrams/bigrams (do not use add-one smoothing to account for them).\n",
    "- Calculate the **sum of logarithms** to avoid issues with underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brave-determination",
   "metadata": {
    "id": "brave-determination"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for category 0: [-62.52129967 -70.58122794]\n",
      "Probability for category 1: [-78.10925023 -66.73064954]\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"United Kingdom officially exits the European Union\",\n",
    " \"How to Lose a Guy in 10 Days\"]\n",
    "prob1 = naive_bayes.calculate_prob(test_docs, 0)\n",
    "prob2 = naive_bayes.calculate_prob(test_docs, 1)\n",
    "print(f\"Probability for category 0: {prob1}\")\n",
    "print(f\"Probability for category 1: {prob2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-sheep",
   "metadata": {
    "id": "occupied-sheep"
   },
   "source": [
    "### C.2.5 Predict labels for new headlines [2 points]\n",
    "With the posterior probability of each category, you can predict the label for new headlines. **Fill in** the function ```predict``` that, given a list of headlines, returns the predicted categories of the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opposed-territory",
   "metadata": {
    "id": "opposed-territory"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "preds = naive_bayes.predict(test_docs)\n",
    "print(f\"Prediction: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-semester",
   "metadata": {
    "id": "bronze-semester"
   },
   "source": [
    "### C.2.6 Calculate evaluation metrics [5 points]\n",
    "To evaluate a classifier, you need to calculate some evaluation metrics. **Fill in** the function ```evaluate``` that, given a list of predictions and a list of true labels, returns the accuracy, macro f1-score, and micro f1-score. You can **NOT** use functions in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-casino",
   "metadata": {
    "id": "level-casino"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Macro f1: 0.7083333333333333\n",
      "Micro f1: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "predictions = [1,1,0,1,0,0,1]\n",
    "labels = [1,0,0,1,0,1,1]\n",
    "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Macro f1: {mac_f1}\")\n",
    "print(f\"Micro f1: {mic_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-corner",
   "metadata": {
    "id": "warming-corner"
   },
   "source": [
    "### C.2.7 Test classifier on test data [2 points]\n",
    "Finally, you are ready to evaluate your classifier on the test data! Run the following cell to make predictions and print out performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amended-angle",
   "metadata": {
    "id": "amended-angle"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.959\n",
      "Macro f1: 0.9589999589999589\n",
      "Micro f1: 0.959\n"
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes.predict(test.text.tolist())\n",
    "labels = test.label.tolist()\n",
    "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Macro f1: {mac_f1}\")\n",
    "print(f\"Micro f1: {mic_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eabcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
